# How-Over-fitting-and-underfitting-effect-the-model-performance
## Overfitting and Underfitting in Machine Learning
## Introduction
This project explores overfitting and underfitting in machine learning, using the MNIST dataset. It demonstrates the impact of techniques like normalization, dropout, regularization, and early stopping on improving model generalization and accuracy.
## Key Results
- Normalization: Stabilizes training, achieving >90% validation accuracy.
- Dropout: Reduces overfitting, improving generalization.
- Regularization (L1, L2, L1_L2): Balances model complexity and performance.
- Early Stopping: Prevents overfitting by halting training early.
- K-Fold Cross Validation: Ensures model robustness across data splits.
